{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1])\n",
      "torch.Size([32, 100, 1])\n",
      "Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(32, 100, 1)  \n",
    "print(a.size())\n",
    "m = nn.Conv1d(100, 100, 1) \n",
    "out = m(a)\n",
    "print(out.size())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "a shape: torch.Size([1, 1, 10])\n",
      "m: Conv1d(1, 2, kernel_size=(3,), stride=(1,), bias=False)\n",
      "m shape: torch.Size([2, 1, 3])\n",
      "Parameter containing:\n",
      "tensor([[[0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000]]], requires_grad=True)\n",
      "out: torch.Size([1, 2, 8])\n",
      "torch.Size([1, 2, 8])\n",
      "tensor([[[ 1.5000,  3.0000,  4.5000,  6.0000,  7.5000,  9.0000, 10.5000,\n",
      "          12.0000],\n",
      "         [ 3.0000,  6.0000,  9.0000, 12.0000, 15.0000, 18.0000, 21.0000,\n",
      "          24.0000]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "length = 10\n",
    "in_channels = 1\n",
    "out_channels1 = 2\n",
    "out_channels2 = 4\n",
    "kernel_size = 3\n",
    "# (N, C_in, L)\n",
    "a = torch.Tensor(np.arange(0, length*batch_size*in_channels).reshape(batch_size, in_channels, length))\n",
    "print(a)\n",
    "print('a shape: {}'.format(a.shape))\n",
    "m = nn.Conv1d(in_channels, out_channels, kernel_size, bias=False)\n",
    "nn.init.constant_(m.weight, 1)\n",
    "nn.init.constant_(m.weight[0], 0.5)\n",
    "print('m: {}'.format(m))\n",
    "print('m shape: {}'.format(m.weight.shape))\n",
    "print(m.weight)\n",
    "with torch.no_grad():\n",
    "    out = m(a)\n",
    "print('out: {}'.format(out.shape))\n",
    "print(out.size())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.]],\n",
      "\n",
      "         [[ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]]]])\n",
      "input shape: torch.Size([1, 2, 3, 3])\n",
      "conv1: Parameter containing:\n",
      "tensor([[[[0.2500, 0.2500],\n",
      "          [0.2500, 0.2500]],\n",
      "\n",
      "         [[0.5000, 0.5000],\n",
      "          [0.5000, 0.5000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000],\n",
      "          [1.0000, 1.0000]],\n",
      "\n",
      "         [[2.0000, 2.0000],\n",
      "          [2.0000, 2.0000]]]], requires_grad=True)\n",
      "conv1 shape: torch.Size([2, 2, 2, 2])\n",
      "out1: tensor([[[[ 24.,  27.],\n",
      "          [ 33.,  36.]],\n",
      "\n",
      "         [[ 96., 108.],\n",
      "          [132., 144.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "out1 shape: torch.Size([1, 2, 2, 2])\n",
      "conv2: Parameter containing:\n",
      "tensor([[[[0.2500, 0.2500],\n",
      "          [0.2500, 0.2500]],\n",
      "\n",
      "         [[0.5000, 0.5000],\n",
      "          [0.5000, 0.5000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000],\n",
      "          [1.0000, 1.0000]],\n",
      "\n",
      "         [[2.0000, 2.0000],\n",
      "          [2.0000, 2.0000]]]], requires_grad=True)\n",
      "conv2 shape: torch.Size([2, 2, 2, 2])\n",
      "out2: tensor([[[[ 270.]],\n",
      "\n",
      "         [[1080.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "out2 shape: torch.Size([1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "height = 3 \n",
    "width = 3\n",
    "conv1_in_channels = 2\n",
    "conv1_out_channels = 2\n",
    "conv2_out_channels = 2\n",
    "kernel_size = 2\n",
    "# (N, C_in, H, W) is shape of all tensors. (batch_size, channels, height, width)\n",
    "input = torch.Tensor(np.arange(0, batch_size*height*width*in_channels).reshape(batch_size, in_channels, height, width))\n",
    "conv1 = nn.Conv2d(in_channels, conv1_out_channels, kernel_size, bias=False) # no bias to make calculations easier\n",
    "# set the weights of the convolutions to make the convolutions easier to follow\n",
    "nn.init.constant_(conv1.weight[0][0], 0.25)\n",
    "nn.init.constant_(conv1.weight[0][1], 0.5)\n",
    "nn.init.constant_(conv1.weight[1][0], 1) \n",
    "nn.init.constant_(conv1.weight[1][1], 2) \n",
    "out1 = conv1(input) # compute the convolution\n",
    "\n",
    "conv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size, bias=False)\n",
    "nn.init.constant_(conv2.weight[0][0], 0.25)\n",
    "nn.init.constant_(conv2.weight[0][1], 0.5)\n",
    "nn.init.constant_(conv2.weight[1][0], 1) \n",
    "nn.init.constant_(conv2.weight[1][1], 2) \n",
    "out2 = conv2(out1) # compute the convolution\n",
    "\n",
    "for tensor, name in zip([input, conv1.weight, out1, conv2.weight, out2], ['input', 'conv1', 'out1', 'conv2', 'out2']):\n",
    "    print('{}: {}'.format(name, tensor))\n",
    "    print('{} shape: {}'.format(name, tensor.shape))\n",
    "          \n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "input shape: torch.Size([1, 1, 10])\n",
      "conv1: Parameter containing:\n",
      "tensor([[[ 0.5000,  0.5000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4770, -0.5338]],\n",
      "\n",
      "        [[-0.4857,  0.1509]],\n",
      "\n",
      "        [[-0.1828, -0.6237]]], requires_grad=True)\n",
      "conv1 shape: torch.Size([5, 1, 2])\n",
      "out1: tensor([[[ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,\n",
      "           7.5000,  8.5000],\n",
      "         [ 1.0000,  3.0000,  5.0000,  7.0000,  9.0000, 11.0000, 13.0000,\n",
      "          15.0000, 17.0000],\n",
      "         [-0.5338, -0.5906, -0.6474, -0.7041, -0.7609, -0.8177, -0.8745,\n",
      "          -0.9313, -0.9881],\n",
      "         [ 0.1509, -0.1840, -0.5189, -0.8538, -1.1886, -1.5235, -1.8584,\n",
      "          -2.1932, -2.5281],\n",
      "         [-0.6237, -1.4302, -2.2368, -3.0433, -3.8498, -4.6563, -5.4628,\n",
      "          -6.2693, -7.0759]]], grad_fn=<SqueezeBackward1>)\n",
      "out1 shape: torch.Size([1, 5, 9])\n",
      "conv2: Parameter containing:\n",
      "tensor([[[0.5000, 0.5000],\n",
      "         [0.5000, 0.5000],\n",
      "         [0.5000, 0.5000],\n",
      "         [0.5000, 0.5000],\n",
      "         [0.5000, 0.5000]],\n",
      "\n",
      "        [[1.0000, 1.0000],\n",
      "         [1.0000, 1.0000],\n",
      "         [1.0000, 1.0000],\n",
      "         [1.0000, 1.0000],\n",
      "         [1.0000, 1.0000]]], requires_grad=True)\n",
      "conv2 shape: torch.Size([2, 5, 2])\n",
      "out2: tensor([[[ 1.3943,  3.1961,  4.9979,  6.7997,  8.6015, 10.4034, 12.2052,\n",
      "          14.0070],\n",
      "         [ 2.7886,  6.3922,  9.9958, 13.5995, 17.2031, 20.8067, 24.4103,\n",
      "          28.0140]]], grad_fn=<SqueezeBackward1>)\n",
      "out2 shape: torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "length = 10\n",
    "conv1_in_channels = 2\n",
    "conv1_out_channels = 5\n",
    "conv2_out_channels = 2\n",
    "kernel_size = 2\n",
    "# (N, C_in, H, W) is shape of all tensors. (batch_size, channels, height, width)\n",
    "input = torch.Tensor(np.arange(0, batch_size*length*in_channels).reshape(batch_size, in_channels, length))\n",
    "conv1 = nn.Conv1d(in_channels, conv1_out_channels, kernel_size, bias=False) # no bias to make calculations easier\n",
    "# shape of conv weights is [out_channels, in_channels, kernel_size]\n",
    "# set the weights of the convolutions to make the convolutions easier to follow\n",
    "nn.init.constant_(conv1.weight[0], 0.5)\n",
    "nn.init.constant_(conv1.weight[1], 1) \n",
    "out1 = conv1(input) # compute the convolution\n",
    "\n",
    "conv2 = nn.Conv1d(conv1_out_channels, conv2_out_channels, kernel_size, bias=False)\n",
    "nn.init.constant_(conv2.weight[0], 0.5)\n",
    "nn.init.constant_(conv2.weight[1], 1) \n",
    "out2 = conv2(out1) # compute the convolution\n",
    "\n",
    "for tensor, name in zip([input, conv1.weight, out1, conv2.weight, out2], ['input', 'conv1', 'out1', 'conv2', 'out2']):\n",
    "    print('{}: {}'.format(name, tensor))\n",
    "    print('{} shape: {}'.format(name, tensor.shape))\n",
    "          \n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
